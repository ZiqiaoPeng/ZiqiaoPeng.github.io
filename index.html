<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziqiao Peng</title>
  
  <meta name="author" content="Ziqiao Peng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziqiao Peng</name>
              </p>
              <p>I am a third-year PhD candidate at 
                <a href="https://www.ruc.edu.cn/en">Renmin University of China</a>,
                 supervised by <a href="http://info.ruc.edu.cn/jsky/rtjs/d696a551fefc4b0ab6f90e02b01f3529.htm">Prof. Jun He</a> from Renmin University of China and <a href="https://www.sem.tsinghua.edu.cn/info/1189/32080.htm">Prof. Hongyan Liu</a> from Tsinghua University.
              </p>
              <p>
                I'm actively seeking internship opportunities that align with my research interests. If you know of any openings or have recommendations, I'd greatly appreciate your input.
              </p>               
              <p>
                My areas of focus include AI-generated content, talking head generation, and video generation.
              </p>              
              <p style="text-align:center">
                <a href="mailto:pengziqiao@ruc.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://github.com/ZiqiaoPeng">Github</a>  

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ZiqiaoPeng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ZiqiaoPeng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
<tr onmouseout="omnisync_stop()" onmouseover="omnisync_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="omnisync_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/omnisync/omnisync.mp4" alt="omnisync mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/omnisync/omnisync.png' alt="omnisync" width="320" height="180">
    </div>
    <script type="text/javascript">
      function omnisync_start() {
        document.getElementById('omnisync_image').style.opacity = "1";
      }

      function omnisync_stop() {
        document.getElementById('omnisync_image').style.opacity = "0";
      }
      omnisync_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./omnisync">
      <papertitle>[arXiv 2025] OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Jiwen Liu,
    Haoxian Zhang,
    Xiaoqiang Liu,
    Songlin Tang,
    Pengfei Wan,
    Di Zhang,
    Hongyan Liu,
    Jun He
    <br>
    
    <a href="./OmniSync">Project</a>
    /
    <a href="https://arxiv.org/pdf/2505.21448">arXiv</a>
    <p></p>
    <p>
      We present OmniSync, a universal lip synchronization framework for diverse visual scenarios. 
    </p>
  </td>
</tr>

<tr onmouseout="dualtalk_stop()" onmouseover="dualtalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="dualtalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/dualtalk/dualtalk.mp4" alt="dualtalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/dualtalk/dualtalk.png' alt="dualtalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function dualtalk_start() {
        document.getElementById('dualtalk_image').style.opacity = "1";
      }

      function dualtalk_stop() {
        document.getElementById('dualtalk_image').style.opacity = "0";
      }
      dualtalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./dualtalk">
      <papertitle>[CVPR 2025] DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Yanbo Fan,
    Haoyu Wu,
    Xuan Wang,
    Hongyan Liu,
    Jun He,
    Zhaoxin Fan
    <br>
    
    <a href="./dualtalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2505.18096">arXiv</a>
    <p></p>
    <p>
      We propose a new task -- multi-round dual-speaker interaction for 3D talking head generation -- which requires models to handle and generate both speaking and listening behaviors in continuous conversation.
    </p>
  </td>
</tr>

<tr onmouseout="synctalkpp_stop()" onmouseover="synctalkpp_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="synctalkpp_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/synctalkpp/synctalkpp.mp4" alt="synctalkpp mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/synctalkpp/synctalkpp.png' alt="synctalkpp" width="320" height="180">
    </div>
    <script type="text/javascript">
      function synctalkpp_start() {
        document.getElementById('synctalkpp_image').style.opacity = "1";
      }

      function synctalkpp_stop() {
        document.getElementById('synctalkpp_image').style.opacity = "0";
      }
      synctalkpp_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./synctalk++">
      <papertitle>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Wentao Hu,
    Junyuan Ma,
    Xiangyu Zhu,
    Xiaomei Zhang,
    Hao Zhao,
    Hui Tian,
    Jun He,
    Hongyan Liu,
    Zhaoxin Fan
    <br>
    
    <a href="./synctalk++">Project</a>
    /
    <a href="https://arxiv.org/">arXiv</a>
    <p></p>
    <p>
      We propose a 3DGS-based method to synthesis realistic talking head videos with better OOD quality.
    </p>
  </td>
</tr>

<tr onmouseout="synctalk_stop()" onmouseover="synctalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="synctalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/synctalk/synctalk.mp4" alt="synctalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/synctalk/synctalk.png' alt="synctalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function synctalk_start() {
        document.getElementById('synctalk_image').style.opacity = "1";
      }

      function synctalk_stop() {
        document.getElementById('synctalk_image').style.opacity = "0";
      }
      synctalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./synctalk">
      <papertitle>[CVPR 2024] SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Wentao Hu,
    Yue Shi,
    Xiangyu Zhu,
    Xiaomei Zhang,
    Hao Zhao,
    Jun He,
    Hongyan Liu,
    Zhaoxin Fan
    <br>
    
    <a href="./synctalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2311.17590">arXiv</a>
    /
    <a href="https://github.com/ziqiaopeng/SyncTalk">Code</a>
    <p></p>
    <p>
      We propose a NeRF-based method to synthesis realistic talking head videos.
    </p>
  </td>
</tr>
          
<tr onmouseout="selftalk_stop()" onmouseover="selftalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="selftalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/selftalk/selftalk.mp4" alt="selftalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/selftalk/selftalk.png' alt="selftalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function selftalk_start() {
        document.getElementById('selftalk_image').style.opacity = "1";
      }

      function selftalk_stop() {
        document.getElementById('selftalk_image').style.opacity = "0";
      }
      selftalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./selftalk">
      <papertitle>[ACM MM 2023] SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Yihao Luo,
    Yue Shi,
    Hao Xu,
    Xiangyu Zhu,
    Hongyan Liu,
    Jun He,
    Zhaoxin Fan
    <br>
    
    <a href="./selftalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2306.10799">arXiv</a>
    /
    <a href="https://github.com/psyai-net/SelfTalk_release">Code</a>
    <p></p>
    <p>
      We propose a novel framework called SelfTalk utilizes a cross-modal network system to generate coherent and visually comprehensible 3D talking faces by reducing the domain gap between different modalities.
    </p>
  </td>
</tr>

<tr onmouseout="emotalk_stop()" onmouseover="emotalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="emotalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/emotalk/emotalk.mp4" alt="emotalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/emotalk/emotalk.jpg' alt="emotalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function emotalk_start() {
        document.getElementById('emotalk_image').style.opacity = "1";
      }

      function emotalk_stop() {
        document.getElementById('emotalk_image').style.opacity = "0";
      }
      emotalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./emotalk">
      <papertitle>[ICCV 2023] EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    Haoyu Wu,
    Zhenbo Song,
    Hao Xu,
    Xiangyu Zhu,
    Hongyan Liu,
    Jun He,
    Zhaoxin Fan
    <br>
    
    <a href="./emotalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2303.11089">arXiv</a>
    /
    <a href="https://github.com/psyai-net/EmoTalk_release">Code</a>
    <p></p>
    <p>
      We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation.
    </p>
  </td>
</tr>

        </tbody></table>
        <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin:30px auto 0 auto;">
          <tr>
            <td style="padding:24px 32px;background:#f7f7fa;border-radius:12px;box-shadow:0 2px 8px rgba(0,0,0,0.04);text-align:center;">
              <heading style="font-size:1.2em;color:#333;">Review Service</heading>
              <div style="margin-top:12px;margin-bottom:6px;">
                <span style="font-weight:bold;">Conferences:</span> 
                CVPR, NeurIPS, ICCV, ECCV, ACM MM, ICME, Eurographics
              </div>
              <div style="margin-bottom:0;">
                <span style="font-weight:bold;">Journals:</span> 
                IJCV, TIP, TMM, TOMM, IET Image Processing, IET Computer Vision
              </div>
            </td>
          </tr>
        </table>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated: June 2025
                <br>
                Web page design credit to <a href="https://jonbarron.info" style="font-size: 14px">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
