<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziqiao Peng</title>
  
  <meta name="author" content="Ziqiao Peng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziqiao Peng</name>
              </p>
              <p>I am currently a first-year Ph.D student at 
                <a href="https://www.ruc.edu.cn/en">Renmin University of China</a>,
                 supervised by <a href="http://info.ruc.edu.cn/jsky/szdw/adszycx/bssds/jsjyyjs1/22e455e725db45a3a310bbc0f045c0f1.htm">Prof. Jun He</a> from RUC and <a href="https://www.sem.tsinghua.edu.cn/info/1189/32080.htm">Prof. Hongyan Liu</a> from THU.
              </p>
              <p>
                I am currently interning at <a href="https://www.psyai.com/">Psyche AI Inc.</a> in Beijing. 
                My position is an algorithm researcher, mainly working on talking head related tasks. 
                If you want to join Psyche AI, please contact me.
              </p>
              <p>
                I am interested in computer vision, 3D facial animation, 3D human motion and AI generated content.
              </p>              
              <p style="text-align:center">
                <a href="mailto:pengziqiao@ruc.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://github.com/ZiqiaoPeng">Github</a>  

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ZiqiaoPeng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ZiqiaoPeng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				

<tr onmouseout="selftalk_stop()" onmouseover="selftalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="selftalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/selftalk/selftalk.mp4" alt="selftalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/selftalk/selftalk.png' alt="selftalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function selftalk_start() {
        document.getElementById('selftalk_image').style.opacity = "1";
      }

      function selftalk_stop() {
        document.getElementById('selftalk_image').style.opacity = "0";
      }
      selftalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./selftalk">
      <papertitle>[ACM MM 2023] SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    <strong>Yihao Luo</strong>,
    <strong>Yue Shi</strong>,
    <strong>Hao Xu</strong>,
    <strong>Xiangyu Zhu</strong>,
    <strong>Hongyan Liu</strong>,
    <strong>Jun He</strong>,
    <strong>Zhaoxin Fan</strong>,
    <br>
    
    <a href="./selftalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2306.10799">arXiv</a>
    /
    <a href="https://github.com/psyai-net/SelfTalk_release">Code</a>
    <p></p>
    <p>
      We propose a novel framework called SelfTalk utilizes a cross-modal network system to generate coherent and visually comprehensible 3D talking faces by reducing the domain gap between different modalities.
    </p>
  </td>
</tr>

<tr onmouseout="emotalk_stop()" onmouseover="emotalk_start()" >
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="two" id="emotalk_image" style="display: inline;"> 
      <video  width="320" height="180" muted autoplay loop>
      <source src="images/emotalk/emotalk.mp4" alt="emotalk mp4" width="320" height="180" type="video/mp4">
      </video>
    </div>
      <img src='images/emotalk/emotalk.jpg' alt="emotalk" width="320" height="180">
    </div>
    <script type="text/javascript">
      function emotalk_start() {
        document.getElementById('emotalk_image').style.opacity = "1";
      }

      function emotalk_stop() {
        document.getElementById('emotalk_image').style.opacity = "0";
      }
      emotalk_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="./emotalk">
      <papertitle>[ICCV 2023] EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation</papertitle>
    </a>
    <br>
    <strong>Ziqiao Peng</strong>,
    <strong>Haoyu Wu</strong>,
    <strong>Zhenbo Song</strong>,
    <strong>Hao Xu</strong>,
    <strong>Xiangyu Zhu</strong>,
    <strong>Hongyan Liu</strong>,
    <strong>Jun He</strong>,
    <strong>Zhaoxin Fan</strong>,
    <br>
    
    <a href="./emotalk">Project</a>
    /
    <a href="https://arxiv.org/abs/2303.11089">arXiv</a>
    /
    <a href="https://github.com/psyai-net/EmoTalk_release">Code</a>
    <p></p>
    <p>
      We propose an end-to-end neural network for speech-driven emotion-enhanced 3D facial animation.
    </p>
  </td>
</tr>

        </tbody></table>

					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Last updated: Sep. 2023
                <br>
                Web page design credit to <a href="https://jonbarron.info" style="font-size: 14px">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
