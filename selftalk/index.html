<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="3D Talking Face, Self-Supervision, Commutative Diagram, Lip Reading">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SelfTalk</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ACM MM 2023</h1>
          <h1 class="title is-1 publication-title">SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ziqiaopeng.github.io/">Ziqiao Peng</a><sup>1</sup>,</span>
            <span class="author-block">
              <strong>Yihao Luo</strong><sup>2,3</sup>,</span>
            <span class="author-block">
              <strong>Yue Shi</strong><sup>3</sup>,
            </span>
            <span class="author-block">
              <strong>Hao Xu</strong><sup>3,4</sup>,
            </span>
            <span class="author-block">
              <a href="https://xiangyuzhu-open.github.io/homepage/">Xiangyu Zhu</a><sup>5</sup>
            </span>
            <br>
            <span class="author-block">
              <a href="https://www.sem.tsinghua.edu.cn/info/1189/32080.htm">Hongyan Liu</a><sup>6</sup>,
            </span>
            <span class="author-block">
              <a href="http://info.ruc.edu.cn/jsky/szdw/adszycx/bssds/jsjyyjs1/22e455e725db45a3a310bbc0f045c0f1.htm">Jun He</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://fanzhaoxin666.github.io/">Zhaoxin Fan</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Renmin University of China,</span>
            <span class="author-block"><sup>2</sup>Imperial College London,</span>
            <span class="author-block"><sup>3</sup>Psyche AI Inc.,</span>
            <span class="author-block"><sup>4</sup>The Hong Kong University of Science and Technology,</span>
            <span class="author-block"><sup>5</sup>State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA,</span>
            <span class="author-block"><sup>6</sup>Tsinghua University</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.10799"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="#teaser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/psyai-net/SelfTalk_release"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay loop controls playsinline height="100%">
        <source src="./static/videos/selftalk-arxiv.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>SelfTalk</strong> utilizes a cross-modal network system to generate coherent and visually comprehensible 3D talking faces by reducing the domain gap between different modalities.
      </h2>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Speech-driven 3D face animation technique, extending its applications to various multimedia fields.
             Previous research has generated promising realistic lip movements and facial expressions from audio signals.
             However, traditional regression models solely driven by data face several essential problems, such as difficulties 
             in accessing precise labels and domain gaps between different modalities, leading to unsatisfactory results lacking precision and coherence.
            <br>
            <br>
            To enhance the visual accuracy of generated lip movement while reducing the dependence on labeled data, we propose a novel framework SelfTalk, by involving self-supervision in a cross-modals network system to learn 3D talking faces.
            The framework constructs a network system consisting of three modules: facial animator, speech recognizer, and lip-reading interpreter. 
            The core of SelfTalk is a commutative training diagram that facilitates compatible features exchange among audio, text, and lip shape, enabling our models to learn the intricate connection between these factors. The proposed framework leverages the knowledge learned from the lip-reading interpreter to generate more plausible lip shapes.
            Extensive experiments and user studies demonstrate that our proposed approach achieves state-of-the-art performance both qualitatively and quantitatively.
            We recommend watching the supplementary.

          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    
    <!-- Proposed Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Method</h2>
        <div class="content has-text-justified">
          <img src="./static/videos/selftalk.png" alt="selftalk">
          <p>
            <br>
            Overview of the proposed SelfTalk. We constructed a commutative training diagram consisting of three modules: facial animator, 
            speech recognizer, and lip-reading interpreter. Specifically, given an input audio signal <img style="transform: translateY(0.1em); background: white;" src="../selftalk/svg/A.svg">, the facial animator 
            module extracts the corresponding facial animation <img style="transform: translateY(0.1em); background: white;" src="../selftalk/svg/Y.svg">, which constitutes the core component of our framework.
            The speech recognizer, on the other hand, is capable of producing the corresponding text <img style="transform: translateY(0.1em); background: white;" src="../selftalk/svg/S.svg"> and utilizing it as a 
            ground truth label for supervision. Lastly, the lip-reading interpreter interprets lip movements, produces a text distribution 
            and establishes a constraint using the label from <img style="transform: translateY(0.1em); background: white;" src="../selftalk/svg/E.svg">.

          </p>
          <!-- <p>
            
          </p>
          <p>
            
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Proposed Method. -->  
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{peng2023selftalk,
        title={SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces}, 
        author={Ziqiao Peng and Yihao Luo and Yue Shi and Hao Xu and Xiangyu Zhu and Hongyan Liu and Jun He and Zhaoxin Fan},
        booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
        pages = {5292â€“5301},
        doi = {10.1145/3581783.3611734},
        year={2023}
      }
</code></pre>
  </div>
</section>
 



<footer class="footer">
  <div class="container">
      <div class="content has-text-centered">
          <a class="icon-link" href="./media/SelfTalk.pdf">
              <i class="fas fa-file-pdf"></i>
          </a>
          <a class="icon-link" href="https://github.com/ZiqiaoPeng/selftalk" class="external-link" disabled>
              <i class="fab fa-github"></i>
          </a>
      </div>
      <div class="columns is-centered">
          <div class="column is-8">
              <div class="content">
                  <p style="text-align:center">
                    This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                  </p>
                  <p style="text-align:center">
                    Website source code based on the <a
                    href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
                  </p>

              </div>
          </div>
      </div>
  </div>
</footer>

</body>
</html>
